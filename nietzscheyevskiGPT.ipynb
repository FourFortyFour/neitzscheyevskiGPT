{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# Thus spoke zarathustra concatenated with Dostoyevsky's 'The Idiot'\n",
    "\n",
    "with open('TSZ_input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character count:  2020697\n",
      "Title: Thus Spake Zarathustra\n",
      "       A Book for All and None\n",
      "\n",
      "“Zarathustra” is my brother’s most personal work; it is the history of\n",
      "his most individual experiences, of his friendships, ideals, raptures,\n",
      "bitterest disappointments and sorrows. Above it all, however, there\n",
      "soars, transfiguring it, the image of his greatest hopes and remotest\n",
      "aims. My brother had the figure of Zarathustra in his mind from his very\n",
      "earliest youth: he once told me that even as a child he had dreamt of\n",
      "him. At different periods in his life, he would call this haunter of his\n",
      "dreams by different names; “but in the end,” he declares in a note on\n",
      "the subject, “I had to do a PERSIAN the honour of identifying him with\n",
      "this creature of my fancy. Persians were the first to take a broad and\n",
      "comprehensive view of history. Every series of evolutions, according\n",
      "to them, was presided over by a prophet; and every prophet had his\n",
      "‘Hazar,’--his dynasty of a thousand years.”\n",
      "\n",
      "All Zarathustra’s views, as also his personality,\n"
     ]
    }
   ],
   "source": [
    "print(\"character count: \", len(text))\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÉàçèéê—‘’“”\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# chars = [c for c in chars if c != 'É' and  c != 'à' and c != 'ç' and c != 'è' and c != 'é' and c != 'ê']\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, char to int\n",
    "\n",
    "# In practice a subword tokenization is used, where the size of the available tokens is larger but the list of ints representing a string is shorter\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda w: ''.join([itos[i] for i in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "data = encode(text)\n",
    "data = np.array(data)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data[:train_split]\n",
    "val_data = data[train_split:]\n",
    "# data = tf.Tensor(data, 0,dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 64, 75, 67, 60, 24,  1, 46, 63])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 8\n",
    "xtrain[:chunk_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: [46] target: 64\n",
      "context: [46 64] target: 75\n",
      "context: [46 64 75] target: 67\n",
      "context: [46 64 75 67] target: 60\n",
      "context: [46 64 75 67 60] target: 24\n",
      "context: [46 64 75 67 60 24] target: 1\n",
      "context: [46 64 75 67 60 24  1] target: 46\n",
      "context: [46 64 75 67 60 24  1 46] target: 63\n"
     ]
    }
   ],
   "source": [
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1:chunk_size+1]\n",
    "\n",
    "# transformer will never receive more than chunk_size tokens/inputs at a time\n",
    "# The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size\n",
    "for k in range(chunk_size):\n",
    "    context = x[:k+1]\n",
    "    target = y[k]\n",
    "    print(f'context: {context} target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 1 60 69 70 76 62 63 10]\n",
      " [73  1 75 63 60  1 67 56]\n",
      " [78 60 77 60 73 11 11 63]\n",
      " [67 80  1 64 69  1 56  1]], shape=(4, 8), dtype=int32)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[60 69 70 76 62 63 10  1]\n",
      " [ 1 75 63 60  1 67 56 74]\n",
      " [60 77 60 73 11 11 63 70]\n",
      " [80  1 64 69  1 56  1 58]], shape=(4, 8), dtype=int32)\n",
      "()_________()\n",
      "context: [1] target: 60\n",
      "context: [ 1 60] target: 69\n",
      "context: [ 1 60 69] target: 70\n",
      "context: [ 1 60 69 70] target: 76\n",
      "context: [ 1 60 69 70 76] target: 62\n",
      "context: [ 1 60 69 70 76 62] target: 63\n",
      "context: [ 1 60 69 70 76 62 63] target: 10\n",
      "context: [ 1 60 69 70 76 62 63 10] target: 1\n",
      "context: [73] target: 1\n",
      "context: [73  1] target: 75\n",
      "context: [73  1 75] target: 63\n",
      "context: [73  1 75 63] target: 60\n",
      "context: [73  1 75 63 60] target: 1\n",
      "context: [73  1 75 63 60  1] target: 67\n",
      "context: [73  1 75 63 60  1 67] target: 56\n",
      "context: [73  1 75 63 60  1 67 56] target: 74\n",
      "context: [78] target: 60\n",
      "context: [78 60] target: 77\n",
      "context: [78 60 77] target: 60\n",
      "context: [78 60 77 60] target: 73\n",
      "context: [78 60 77 60 73] target: 11\n",
      "context: [78 60 77 60 73 11] target: 11\n",
      "context: [78 60 77 60 73 11 11] target: 63\n",
      "context: [78 60 77 60 73 11 11 63] target: 70\n",
      "context: [67] target: 80\n",
      "context: [67 80] target: 1\n",
      "context: [67 80  1] target: 64\n",
      "context: [67 80  1 64] target: 69\n",
      "context: [67 80  1 64 69] target: 1\n",
      "context: [67 80  1 64 69  1] target: 56\n",
      "context: [67 80  1 64 69  1 56] target: 1\n",
      "context: [67 80  1 64 69  1 56  1] target: 58\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(4646)\n",
    "batch_size = 4\n",
    "chunk_size = 8\n",
    "def get_batch(dat):\n",
    "    dat = xtrain if dat == 'train' else val_data\n",
    "    randint = tf.random.uniform(shape=(batch_size,), minval=0, maxval=len(dat)-chunk_size-1, dtype=tf.int64)\n",
    "    x = tf.stack([dat[i:i+chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i+1:i+chunk_size+1] for i in randint])\n",
    "    # print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x,y\n",
    "\n",
    "xs, ys = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xs.shape)\n",
    "print(xs)\n",
    "print('targets:')\n",
    "print(ys.shape)\n",
    "print(ys)\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "print('()_________()')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for k in range(chunk_size):\n",
    "        context = xs[b,:k+1]\n",
    "        target = ys[b,k]\n",
    "        print(f'context: {context} target: {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 64)          5952      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 1024)        4460544   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, None, 93)          95325     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,561,821\n",
      "Trainable params: 4,561,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, 64),\n",
    "        tf.keras.layers.LSTM(1024, return_sequences=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    model.summary()\n",
    "    return model\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits.shape: (32, 93), targets.shape: (32,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 93), dtype=float32, numpy=\n",
       "array([[ 0.00926419,  0.01209891,  0.0227098 , ..., -0.00062816,\n",
       "        -0.02765806,  0.01870995],\n",
       "       [ 0.049946  , -0.04618012, -0.02935493, ...,  0.01709548,\n",
       "         0.02467797,  0.03596773],\n",
       "       [ 0.03407184,  0.01388774, -0.04212189, ...,  0.03161688,\n",
       "        -0.01108854, -0.02865429],\n",
       "       ...,\n",
       "       [ 0.00926419,  0.01209891,  0.0227098 , ..., -0.00062816,\n",
       "        -0.02765806,  0.01870995],\n",
       "       [-0.03403344,  0.02255275, -0.00292721, ..., -0.02929988,\n",
       "        -0.04217792,  0.01755125],\n",
       "       [ 0.00926419,  0.01209891,  0.0227098 , ..., -0.00062816,\n",
       "        -0.02765806,  0.01870995]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers, backend\n",
    "from keras.layers import Lambda\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "# import keras.backend as K\n",
    "# rc: https://www.youtube.com/watch?v=PaCmpygFfXo&t=182s -> Bigram model details by Karpathy\n",
    "\n",
    "# N-gram language model https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "class ngramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        # `super` call to inherit from keras.layers.Layer\n",
    "        super(ngramLangModel,self).__init__()\n",
    "\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "    def build(self, input_shape):\n",
    "        # if subclassers need a \"state creation step\"\n",
    "        # This method is used to create weights that depend on input.shape\n",
    "        # __call__ will auto build the layer if it hasn't been built yet\n",
    "\n",
    "        pass\n",
    "\n",
    "    # !inference mode vs training mode\n",
    "    def call(self, xss, targets=None):\n",
    "        # Called in __call__, after build has been called?\n",
    "        # Preforms logic of applying the layer to the input tensors\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        logits = self.token_embedding(xss)\n",
    "\n",
    "        # from logits to normalized probabilities\n",
    "        # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "        \"\"\"\n",
    "        # Figure out dimensions\n",
    "        # the err thrown is \"EagerTensor object has no attribute 'reshape'.\"\n",
    "        # the above enabling of numpy behavior has fixed this, I presume.\n",
    "        \"\"\"\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        B, T, C = logits.shape\n",
    "        logits = tf.reshape(logits, (B*T, C))\n",
    "        targets = tf.reshape(targets, (B*T))\n",
    "        print(f\"logits.shape: {logits.shape}, targets.shape: {targets.shape}\")\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, xss, max_toks):\n",
    "\n",
    "        for _ in range(max_toks):\n",
    "            logits, loss = self(xss)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            next_token = tf.random.categorical(probs, num_samples=1) #(B, 1)\n",
    "            # first dimension/axis is the time dimension\n",
    "            xss = tf.concat([xss, next_token], axis=1)\n",
    "\n",
    "        return xss\n",
    "\n",
    "    # The below func can just be used as a layer. Src: https://blog.codecentric.de/move-n-gram-extraction-into-your-keras-model\n",
    "    # def ngram_block(n, alphabet_size):\n",
    "    #     def wrapped(inputs):\n",
    "    #         layer = layers.Conv1D(1, n, use_bias=False, trainable=False)\n",
    "    #         x = layers.Reshape((-1, 1))(inputs)\n",
    "    #         x = layer(x)\n",
    "    #         kernel = np.power(alphabet_size, range(0, n),\n",
    "    #                           dtype=backend.floatx())\n",
    "    #         layer.set_weights([kernel.reshape(n, 1, 1)])\n",
    "    #         return layers.Reshape((-1,))(x)\n",
    "\n",
    "    #     return wrapped\n",
    "\n",
    "    def bigram(self, data):\n",
    "        return tf.convert_to_tensor(Lambda(lambda x: [x[:,:-1] + x[:,1:] * vocab_size])(data))\n",
    "\n",
    "\n",
    "\n",
    "ngram = ngramLangModel(vocab_size)\n",
    "\n",
    "out,loss = ngram(xs,ys)\n",
    "# type(bgramd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on gpu\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = get_model()\n",
    "#     model.compile()\n",
    "#     model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
