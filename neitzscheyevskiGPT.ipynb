{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided by Andrej Karpathy: https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "# Thus spoke zarathustra concatenated with Dostoyevsky's 'The Idiot'\n",
    "\n",
    "with open('TSZ_input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character count:  2020697\n",
      "Title: Thus Spake Zarathustra\n",
      "       A Book for All and None\n",
      "\n",
      "“Zarathustra” is my brother’s most personal work; it is the history of\n",
      "his most individual experiences, of his friendships, ideals, raptures,\n",
      "bitterest disappointments and sorrows. Above it all, however, there\n",
      "soars, transfiguring it, the image of his greatest hopes and remotest\n",
      "aims. My brother had the figure of Zarathustra in his mind from his very\n",
      "earliest youth: he once told me that even as a child he had dreamt of\n",
      "him. At different periods in his life, he would call this haunter of his\n",
      "dreams by different names; “but in the end,” he declares in a note on\n",
      "the subject, “I had to do a PERSIAN the honour of identifying him with\n",
      "this creature of my fancy. Persians were the first to take a broad and\n",
      "comprehensive view of history. Every series of evolutions, according\n",
      "to them, was presided over by a prophet; and every prophet had his\n",
      "‘Hazar,’--his dynasty of a thousand years.”\n",
      "\n",
      "All Zarathustra’s views, as also his personality,\n"
     ]
    }
   ],
   "source": [
    "print(\"character count: \", len(text))\n",
    "\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"$%'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_abcdefghijklmnopqrstuvwxyzÉàçèéê—‘’“”\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "# chars = [c for c in chars if c != 'É' and  c != 'à' and c != 'ç' and c != 'è' and c != 'é' and c != 'ê']\n",
    "\n",
    "print(''.join(chars))\n",
    "print(vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization, char to int\n",
    "\n",
    "# In practice a subword tokenization is used, where the size of the available tokens is larger but the list of ints representing a string is shorter\n",
    "stoi = {c:i for i,c in enumerate(chars)}\n",
    "itos = {i:c for i,c in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda w: ''.join([itos[i] for i in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "data = encode(text)\n",
    "data = np.array(data)\n",
    "\n",
    "train_split = int(len(data) * 0.9)\n",
    "xtrain = data[:train_split]\n",
    "val_data = data[train_split:]\n",
    "# data = tf.Tensor(data, 0,dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 64, 75, 67, 60, 24,  1, 46, 63])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 8\n",
    "xtrain[:chunk_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context: [46] target: 64\n",
      "context: [46 64] target: 75\n",
      "context: [46 64 75] target: 67\n",
      "context: [46 64 75 67] target: 60\n",
      "context: [46 64 75 67 60] target: 24\n",
      "context: [46 64 75 67 60 24] target: 1\n",
      "context: [46 64 75 67 60 24  1] target: 46\n",
      "context: [46 64 75 67 60 24  1 46] target: 63\n"
     ]
    }
   ],
   "source": [
    "x = xtrain[:chunk_size]\n",
    "y = xtrain[1:chunk_size+1]\n",
    "\n",
    "# transformer will never receive more than chunk_size tokens/inputs at a time\n",
    "# The below operation is not only done for efficiency but also so the transformer is 'used' to seeing context of size 1..chunk_size\n",
    "for k in range(chunk_size):\n",
    "    context = x[:k+1]\n",
    "    target = y[k]\n",
    "    print(f'context: {context} target: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[ 1 60 69 70 76 62 63 10]\n",
      " [73  1 75 63 60  1 67 56]\n",
      " [78 60 77 60 73 11 11 63]\n",
      " [67 80  1 64 69  1 56  1]], shape=(4, 8), dtype=int32)\n",
      "targets:\n",
      "(4, 8)\n",
      "tf.Tensor(\n",
      "[[60 69 70 76 62 63 10  1]\n",
      " [ 1 75 63 60  1 67 56 74]\n",
      " [60 77 60 73 11 11 63 70]\n",
      " [80  1 64 69  1 56  1 58]], shape=(4, 8), dtype=int32)\n",
      "()_________()\n",
      "context: [1] target: 60\n",
      "context: [ 1 60] target: 69\n",
      "context: [ 1 60 69] target: 70\n",
      "context: [ 1 60 69 70] target: 76\n",
      "context: [ 1 60 69 70 76] target: 62\n",
      "context: [ 1 60 69 70 76 62] target: 63\n",
      "context: [ 1 60 69 70 76 62 63] target: 10\n",
      "context: [ 1 60 69 70 76 62 63 10] target: 1\n",
      "context: [73] target: 1\n",
      "context: [73  1] target: 75\n",
      "context: [73  1 75] target: 63\n",
      "context: [73  1 75 63] target: 60\n",
      "context: [73  1 75 63 60] target: 1\n",
      "context: [73  1 75 63 60  1] target: 67\n",
      "context: [73  1 75 63 60  1 67] target: 56\n",
      "context: [73  1 75 63 60  1 67 56] target: 74\n",
      "context: [78] target: 60\n",
      "context: [78 60] target: 77\n",
      "context: [78 60 77] target: 60\n",
      "context: [78 60 77 60] target: 73\n",
      "context: [78 60 77 60 73] target: 11\n",
      "context: [78 60 77 60 73 11] target: 11\n",
      "context: [78 60 77 60 73 11 11] target: 63\n",
      "context: [78 60 77 60 73 11 11 63] target: 70\n",
      "context: [67] target: 80\n",
      "context: [67 80] target: 1\n",
      "context: [67 80  1] target: 64\n",
      "context: [67 80  1 64] target: 69\n",
      "context: [67 80  1 64 69] target: 1\n",
      "context: [67 80  1 64 69  1] target: 56\n",
      "context: [67 80  1 64 69  1 56] target: 1\n",
      "context: [67 80  1 64 69  1 56  1] target: 58\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(4646)\n",
    "batch_size = 4\n",
    "chunk_size = 8\n",
    "def get_batch(dat):\n",
    "    dat = xtrain if dat == 'train' else val_data\n",
    "    randint = tf.random.uniform(shape=(batch_size,), minval=0, maxval=len(dat)-chunk_size-1, dtype=tf.int64)\n",
    "    x = tf.stack([dat[i:i+chunk_size] for i in randint])\n",
    "    y = tf.stack([dat[i+1:i+chunk_size+1] for i in randint])\n",
    "    # print(f\"randint: {randint}, randint.shape: {randint.shape}\")\n",
    "    return x,y\n",
    "\n",
    "xs, ys = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xs.shape)\n",
    "print(xs)\n",
    "print('targets:')\n",
    "print(ys.shape)\n",
    "print(ys)\n",
    "\"\"\"\n",
    "x=[1,60,69..10]\n",
    "y = [60,69,..1]\n",
    "input x[0,0:2]= [1,60] output is y[2] = 69\n",
    "\"\"\"\n",
    "\n",
    "print('()_________()')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for k in range(chunk_size):\n",
    "        context = xs[b,:k+1]\n",
    "        target = ys[b,k]\n",
    "        print(f'context: {context} target: {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 60, 69, 70, 76, 62, 63, 10],\n",
       "       [73,  1, 75, 63, 60,  1, 67, 56],\n",
       "       [78, 60, 77, 60, 73, 11, 11, 63],\n",
       "       [67, 80,  1, 64, 69,  1, 56,  1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def ngram(n, data):\n",
    "#     return [data[i:i+n] for i in range(len(data)-n+1)]\n",
    "# tf.keras.layers.TextVectorization(...)?\n",
    "# xs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 8, 93])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# import keras.backend as K\n",
    "\n",
    "# N-gram language model https://web.stanford.edu/~jurafsky/slp3/3.pdf\n",
    "class ngramLangModel(keras.layers.Layer):\n",
    "    def __init__(self, vocab_size):\n",
    "        # `super` call to inherit from keras.layers.Layer\n",
    "        super(ngramLangModel, self).__init__()\n",
    "\n",
    "        self.token_embedding = keras.layers.Embedding(vocab_size, vocab_size)\n",
    "    # state init\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    # every row from input xs will refer to embedding table, and index a row that corresponds to the token\n",
    "    # need to be arranged in a (Batch, Time, Channel?) format --> (4,8,vocab_size)\n",
    "    def call(self, xss, targets):\n",
    "        logits = self.token_embedding(xss)\n",
    "        # from logits to normalized probabilities\n",
    "        # CategoricalCrossentropy expects labels to be provided in a one hot rep, labels as ints use SparseCategoricalCrossentropy\n",
    "        \"\"\"\n",
    "        # Figure out dimensions\n",
    "\n",
    "        loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.reshape(-1)\n",
    "        targets = targets.reshape(-1)\n",
    "        \"\"\"\n",
    "\n",
    "        # self.add_loss(loss(logits, targets))\n",
    "        return logits\n",
    "\n",
    "b = ngramLangModel(vocab_size)\n",
    "out = b(xs, ys)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on gpu\n",
    "# with tf.device('/GPU:0'):\n",
    "#     model = get_model()\n",
    "#     model.compile()\n",
    "#     model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
